{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb158905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from random import sample, shuffle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from src.DataGenerator import AudioDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Reshape, Conv2DTranspose, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from src.helper_functions import plot_reconstruction\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "394abc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98000 files for Training set\n",
      "Found 2000 files for Test set\n"
     ]
    }
   ],
   "source": [
    "data_gen = AudioDataGenerator(\n",
    "    directory='data/Spotify/comp_pngs/', \n",
    "    image_size=(128,512), \n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    sample_size=100000,\n",
    "    shuffle=True,\n",
    "    train_test_split=True, \n",
    "    test_size=0.02,\n",
    "    output_channel_index=0,\n",
    "    output_size=(128,128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be2fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 128\n",
    "img_height = 128\n",
    "kernel_size = 5\n",
    "strides = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a1efaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Freq_Autoencoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, num_channels):\n",
    "        super(Time_Freq_Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.time_encoder = Sequential([\n",
    "            Reshape(target_shape=(128,128)),\n",
    "            tf.keras.layers.Conv1D(input_shape=(img_height, img_width), filters=64, kernel_size=kernel_size, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            tf.keras.layers.Conv1D(filters=64, kernel_size=kernel_size, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            tf.keras.layers.Conv1D(filters=128, kernel_size=kernel_size, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            tf.keras.layers.Conv1D(filters=256, kernel_size=kernel_size, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            Flatten(),\n",
    "            Dense(2048, activation='relu'),\n",
    "            Dense(units=latent_dim//2)\n",
    "        ])\n",
    "        self.freq_encoder = Sequential([\n",
    "            Reshape(target_shape=(128,128)),\n",
    "            tf.keras.layers.Conv1D(input_shape=(img_height, img_width), filters=64, kernel_size=5, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            tf.keras.layers.Conv1D(filters=64, kernel_size=kernel_size, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            tf.keras.layers.Conv1D(filters=128, kernel_size=kernel_size, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            tf.keras.layers.Conv1D(filters=256, kernel_size=kernel_size, padding=\"same\", strides=strides, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            Flatten(),\n",
    "            Dense(2048, activation='relu'),\n",
    "            Dense(units=latent_dim//2)\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim)),\n",
    "            Dense(units=16384, activation='relu'),\n",
    "            BatchNormalization(axis=-1),\n",
    "            Reshape(target_shape=(8,8,256)),\n",
    "            Conv2DTranspose(filters=256, kernel_size=kernel_size, strides=strides, padding=\"same\", activation=\"relu\"),\n",
    "            BatchNormalization(axis=-1),\n",
    "            Conv2DTranspose(filters=128, kernel_size=kernel_size, strides=strides, padding=\"same\", activation=\"relu\"),\n",
    "            BatchNormalization(axis=-1),\n",
    "            Conv2DTranspose(filters=64, kernel_size=kernel_size,  strides=strides, padding=\"same\", activation=\"relu\"),\n",
    "            BatchNormalization(axis=-1),\n",
    "            Conv2DTranspose(filters=32, kernel_size=kernel_size,  strides=strides, padding=\"same\", activation=\"relu\"),\n",
    "            BatchNormalization(axis=-1),\n",
    "            Conv2DTranspose(filters=num_channels, kernel_size=kernel_size, padding=\"same\", activation='sigmoid'),\n",
    "        ])\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_1 = x\n",
    "        x_2 = tf.transpose(x, perm=[0,2,1,3])\n",
    "        encoded_time = self.time_encoder(x_1)\n",
    "        encoded_freq = self.freq_encoder(x_2)\n",
    "        encoded = tf.keras.layers.Concatenate(axis=1)([encoded_time, encoded_freq])\n",
    "        return encoded\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "autoencoder = Time_Freq_Autoencoder(256, 1)\n",
    "\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "\n",
    "autoencoder.compile(optimizer=opt, loss=tf.keras.losses.mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = autoencoder.fit(data_gen.train,\n",
    "#                        batch_size=data_gen.batch_size,\n",
    "#                        epochs=10,\n",
    "#                        validation_data=data_gen.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20211606",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_2 = autoencoder.fit(data_gen.train,\n",
    "                       batch_size=data_gen.batch_size,\n",
    "                       epochs=5,\n",
    "                       validation_data=data_gen.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e91bc28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/autoencoder_512dim_time_freq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/autoencoder_512dim_time_freq/assets\n"
     ]
    }
   ],
   "source": [
    "autoencoder_path = 'data/autoencoder_512dim_time_freq'\n",
    "autoencoder.save(autoencoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a2fbb2f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_52\" (type Sequential).\n\nA `Concatenate` layer should be called on a list of at least 1 input. Received: input_shape=(32, 128, 128, 1)\n\nCall arguments received by layer \"sequential_52\" (type Sequential):\n  • inputs=tf.Tensor(shape=(32, 128, 128, 1), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_img \u001b[38;5;241m=\u001b[39m data_gen\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m20\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plot_reconstruction(test_img, prediction, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/golden_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36mTime_Freq_Autoencoder.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoded)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_52\" (type Sequential).\n\nA `Concatenate` layer should be called on a list of at least 1 input. Received: input_shape=(32, 128, 128, 1)\n\nCall arguments received by layer \"sequential_52\" (type Sequential):\n  • inputs=tf.Tensor(shape=(32, 128, 128, 1), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "test_img = data_gen.take(20)[0]\n",
    "prediction = autoencoder(test_img)\n",
    "\n",
    "plot_reconstruction(test_img, prediction, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dfbf908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 256), dtype=float32, numpy=\n",
       "array([[ 911.20306, -862.2692 , -397.7018 , ..., -863.3008 , -368.2759 ,\n",
       "        -526.147  ],\n",
       "       [ 891.67053, -904.7645 , -444.9764 , ..., -939.6398 , -309.77386,\n",
       "        -780.13275],\n",
       "       [1043.9781 , -926.9036 , -317.7887 , ..., -856.2506 , -233.06314,\n",
       "        -683.01117],\n",
       "       ...,\n",
       "       [ 707.19745, -734.52325, -285.39926, ..., -927.082  , -312.66   ,\n",
       "        -624.9271 ],\n",
       "       [ 879.4098 , -906.7295 , -463.8893 , ..., -963.3282 , -493.59512,\n",
       "        -707.8858 ],\n",
       "       [ 871.25256, -794.2508 , -569.2462 , ..., -954.0515 , -423.87006,\n",
       "        -680.7016 ]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.encoder(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed6deb51",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLatentSpace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatentSpace\n\u001b[0;32m----> 3\u001b[0m latent_space \u001b[38;5;241m=\u001b[39m \u001b[43mLatentSpace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoencoder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/Spotify/comp_pngs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtracks_feather_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/all_tracks.feather\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Flatiron/Projects/Music-Genre-Latent-Space/src/LatentSpace.py:17\u001b[0m, in \u001b[0;36mLatentSpace.__init__\u001b[0;34m(self, autoencoder_path, image_dir, tracks_feather_path, sample_size, latent_dims, num_channels, output_size, scale)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, autoencoder_path, image_dir, tracks_feather_path, sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latent_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m), scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_generator \u001b[38;5;241m=\u001b[39m AudioDataGenerator(directory\u001b[38;5;241m=\u001b[39mimage_dir,\n\u001b[1;32m     19\u001b[0m                                 image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m512\u001b[39m),\n\u001b[1;32m     20\u001b[0m                                 color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                 num_output_channels\u001b[38;5;241m=\u001b[39mnum_channels,\n\u001b[1;32m     26\u001b[0m                                 output_size\u001b[38;5;241m=\u001b[39moutput_size)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(latent_dims)]\n",
      "File \u001b[0;32m~/miniforge3/envs/golden_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/golden_env/lib/python3.9/site-packages/keras/utils/generic_utils.py:707\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    705\u001b[0m   obj \u001b[38;5;241m=\u001b[39m _GLOBAL_CUSTOM_OBJECTS[object_name]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m   obj \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(object_name)\n\u001b[1;32m    708\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please ensure \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis object is passed to the `custom_objects` argument. See \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from src.LatentSpace import LatentSpace\n",
    "\n",
    "latent_space = LatentSpace(autoencoder_path=autoencoder_path,\n",
    "                        image_dir='data/Spotify/comp_pngs/',\n",
    "                        tracks_feather_path='data/all_tracks.feather', latent_dims=512, output_size=(128, 128)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a632b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from autoencoder...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Time_Freq_Autoencoder' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlatent_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/autoencoder_512dim_time_freq\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Flatiron/Projects/Music-Genre-Latent-Space/src/LatentSpace.py:42\u001b[0m, in \u001b[0;36mLatentSpace.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(search_range):\n\u001b[1;32m     41\u001b[0m     filename, latent_img, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_generator\u001b[38;5;241m.\u001b[39mtake(i, return_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, get_all_tiles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 42\u001b[0m     latent_space \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m(latent_img))\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(latent_space)):\n\u001b[1;32m     46\u001b[0m         result\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mstr\u001b[39m(filename[j])\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mstr\u001b[39m(filename[j]),\n\u001b[1;32m     49\u001b[0m               }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Time_Freq_Autoencoder' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "latent_space.build()\n",
    "\n",
    "try:\n",
    "    os.mkdir('data/autoencoder_512dim_time_freq')\n",
    "except:\n",
    "    pass\n",
    "latent_space.save('data/autoencoder_512dim_time_freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a3888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
